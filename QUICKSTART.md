# å¿«é€Ÿå¼€å§‹æŒ‡å—

## ğŸ“‹ å‰ç½®æ¡ä»¶

```bash
# å·²å®‰è£…çš„ä¾èµ–ï¼ˆå‚è€ƒ pyproject.tomlï¼‰
pip install torch torchvision datasets hydra-core loguru rich
```

## ğŸš€ 5 åˆ†é’Ÿå¿«é€Ÿå¼€å§‹

### 1. æŸ¥çœ‹ç¤ºä¾‹ä»£ç 

```bash
# è¿è¡Œç¤ºä¾‹è„šæœ¬ï¼Œäº†è§£å„ä¸ªç»„ä»¶å¦‚ä½•å·¥ä½œ
python -m classifier.example_usage
```

è¿™ä¼šå±•ç¤ºï¼š
- âœ… Processor å¦‚ä½•å¤„ç†å•ä¸ªå›¾åƒ
- âœ… Collator å¦‚ä½•æ‰¹å¤„ç†
- âœ… Model å¦‚ä½•è¿›è¡Œæ¨ç†
- âœ… ç»„ä»¶å¦‚ä½•ååŒå·¥ä½œ

### 2. å¼€å§‹è®­ç»ƒ

#### é»˜è®¤é…ç½®è®­ç»ƒ

```bash
python -m classifier.train
```

è¿™ä¼šä½¿ç”¨é»˜è®¤é…ç½®ï¼š
- æ•°æ®é›†: Flowers102
- æ¨¡å‹: ConvNeXt Tiny
- Batch size: 32
- Epochs: 100

#### å¿«é€Ÿè°ƒè¯•ï¼ˆä½¿ç”¨å°‘é‡æ•°æ®ï¼‰

```bash
python -m classifier.train \
    dataset.max_train_samples=100 \
    dataset.max_val_samples=50 \
    trainer.epochs=3 \
    trainer.log_interval=1
```

#### è‡ªå®šä¹‰é…ç½®

```bash
# ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹
python -m classifier.train model=convnext_small

# ä½¿ç”¨ ViT Base é¢„è®¾ï¼ˆè„šæœ¬ä¼šè‡ªåŠ¨æ¿€æ´» .venvï¼‰
./scripts/train/vit.sh \
    trainer.train_batch_size=32 \
    trainer.learning_rate=3e-5

# è°ƒæ•´è¶…å‚æ•°
python -m classifier.train \
    trainer.batch_size=64 \
    trainer.optimizer.lr=5e-4 \
    trainer.epochs=50

# ä¿®æ”¹æ•°æ®å¢å¼º
python -m classifier.train \
    model.train_processor.hflip_prob=0.8 \
    model.train_processor.color_jitter=false
```

### 3. æŸ¥çœ‹ç»“æœ

è®­ç»ƒè¾“å‡ºä¿å­˜åœ¨ `outputs/YYYY-MM-DD/HH-MM-SS/`:

```
outputs/2024-01-01/12-00-00/
â”œâ”€â”€ config.yaml          # å®Œæ•´é…ç½®
â”œâ”€â”€ best_model.pth      # æœ€ä½³æ¨¡å‹
â””â”€â”€ last_model.pth      # æœ€åçš„æ¨¡å‹
```

åŠ è½½æ¨¡å‹:

```python
import torch
from classifier.models import ConvNeXtModel

# åˆ›å»ºæ¨¡å‹
model = ConvNeXtModel("convnext_tiny", num_classes=102)

# åŠ è½½æƒé‡
checkpoint = torch.load("outputs/.../best_model.pth")
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()
```

### 4. è¿›è¡Œè¯„æµ‹

```bash
# å•å¡è¯„æµ‹å¤šä¸ªæ£€æŸ¥ç‚¹ï¼ˆæ”¯æŒä»»æ„æ•°é‡ï¼Œä½¿ç”¨ -- åˆ†éš”é¢å¤– Hydra å‚æ•°ï¼‰
./scripts/eval/single.sh \
    outputs/resnet152_lr1e-5/step_300 \
    outputs/resnet152_lr1e-5/step_400 \
    -- evaluation.split=test evaluation.metrics_output_path=outputs/test_metrics.json

# åˆ†å¸ƒå¼è¯„æµ‹ï¼ˆFSDP æ£€æŸ¥ç‚¹éœ€ä¸è®­ç»ƒ world size ç›¸åŒï¼‰
NPROC_PER_NODE=4 ./scripts/eval/distributed.sh \
    outputs/resnet152_lr1e-5/step_500 \
    outputs/resnet152_lr1e-5/step_450 \
    -- evaluation.topk=[1,3,5]

# ä»å¯ç›´æ¥è°ƒç”¨ Hydra å…¥å£ï¼ˆæ— è„šæœ¬åœºæ™¯ï¼‰
python -m eval.pipeline.run evaluation.checkpoint_path=outputs/resnet152_lr1e-5/step_500
```

- `evaluation.split`ï¼š`train` / `validation` / `test` æˆ–ä»»æ„ HF splitï¼›é»˜è®¤ `validation`ã€‚
- `evaluation.max_samples`ï¼šé™åˆ¶æ ·æœ¬æ•°é‡ï¼Œå¯åŠ é€ŸæŠ½æŸ¥ã€‚
- `evaluation.metrics_output_path`ï¼šæä¾› JSON è·¯å¾„å³å¯è‡ªåŠ¨å†™å‡ºæŒ‡æ ‡ã€‚
- `CHECKPOINT_FORMAT` ç¯å¢ƒå˜é‡æ§åˆ¶è„šæœ¬åŠ è½½æ–¹å¼ï¼ˆå•å¡é»˜è®¤ autoï¼Œå¤šå¡é»˜è®¤ fsdpï¼‰ã€‚
- ä½¿ç”¨ FSDP åˆ†ç‰‡æƒé‡æ—¶ï¼Œ`torchrun` è¯„æµ‹çš„ `NPROC_PER_NODE` å¿…é¡»ä¸è®­ç»ƒä¸€è‡´ã€‚

## ğŸ“š é…ç½®è¯´æ˜

### æ•°æ®é›†é…ç½® (`dataset=...`)

åœ¨ `classifier/conf/dataset/` ä¸­å®šä¹‰ï¼š

```yaml
# flowers102.yaml
dataset_id: pufanyi/flowers102
num_classes: 102
max_train_samples: null  # é™åˆ¶è®­ç»ƒæ ·æœ¬æ•°ï¼ˆè°ƒè¯•ç”¨ï¼‰
```

### æ¨¡å‹é…ç½® (`model=...`)

åœ¨ `classifier/conf/model/` ä¸­å®šä¹‰ï¼š

```yaml
# convnext_tiny.yaml
name: convnext_tiny
pretrained: true
dropout: 0.1

processor:
  image_size: 224
  resize_size: 256

train_processor:
  image_size: 224
  hflip_prob: 0.5
  color_jitter: true
```

### è®­ç»ƒé…ç½® (`trainer.*`)

åœ¨ `classifier/conf/trainer/default.yaml` ä¸­å®šä¹‰ï¼š

```yaml
epochs: 100
batch_size: 32
num_workers: 4

optimizer:
  name: adamw
  lr: 1.0e-4
  weight_decay: 1.0e-4

scheduler:
  name: cosine
  warmup_epochs: 5
```

## ğŸ¯ å¸¸è§ä»»åŠ¡

### ä»»åŠ¡ 1ï¼šæ·»åŠ æ–°çš„æ•°æ®é›†

1. åˆ›å»ºé…ç½®æ–‡ä»¶ `classifier/conf/dataset/my_dataset.yaml`:

```yaml
name: my_dataset
dataset_id: username/my_dataset  # HuggingFace dataset
num_classes: 10

train_split: train
val_split: validation
test_split: test

image_column: image
label_column: label
```

2. è¿è¡Œè®­ç»ƒ:

```bash
python -m classifier.train dataset=my_dataset
```

### ä»»åŠ¡ 2ï¼šè°ƒæ•´æ•°æ®å¢å¼º

```bash
# æ›´æ¿€è¿›çš„å¢å¼º
python -m classifier.train \
    model.train_processor.scale=[0.5,1.0] \
    model.train_processor.hflip_prob=0.8 \
    model.train_processor.color_jitter=true

# å…³é—­å¢å¼º
python -m classifier.train \
    model.train_processor.hflip_prob=0.0 \
    model.train_processor.color_jitter=false
```

### ä»»åŠ¡ 3ï¼šä¿®æ”¹ä¼˜åŒ–å™¨è®¾ç½®

```bash
# ä½¿ç”¨æ›´å¤§çš„å­¦ä¹ ç‡
python -m classifier.train trainer.optimizer.lr=1e-3

# ä¿®æ”¹å­¦ä¹ ç‡è°ƒåº¦
python -m classifier.train \
    trainer.scheduler.warmup_epochs=10 \
    trainer.scheduler.min_lr=1e-7
```

### ä»»åŠ¡ 4ï¼šä½¿ç”¨ä¸åŒçš„æ¨¡å‹å¤§å°

```bash
# ConvNeXt Small
python -m classifier.train model=convnext_small

# ConvNeXt Base (éœ€è¦æ›´å¤šæ˜¾å­˜)
python -m classifier.train model=convnext_base trainer.batch_size=16
```

### ä»»åŠ¡ 5ï¼šæ¢å¤è®­ç»ƒ

```python
# åœ¨ Python è„šæœ¬ä¸­
checkpoint = torch.load("outputs/.../checkpoint_epoch_50.pth")
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
start_epoch = checkpoint['epoch']
best_acc = checkpoint['best_acc']

# ç„¶åç»§ç»­è®­ç»ƒ...
```

## ğŸ”§ å¼€å‘å’Œè°ƒè¯•

### å¿«é€Ÿæµ‹è¯•æµæ°´çº¿

```bash
# ä½¿ç”¨ 10 ä¸ªæ ·æœ¬å¿«é€Ÿæµ‹è¯•
python -m classifier.train \
    dataset.max_train_samples=10 \
    dataset.max_val_samples=5 \
    trainer.epochs=2 \
    trainer.batch_size=2 \
    trainer.num_workers=0
```

### æŸ¥çœ‹é…ç½®

```bash
# Hydra ä¼šæ‰“å°å®Œæ•´çš„åˆå¹¶åçš„é…ç½®
python -m classifier.train --cfg job
```

### è¦†ç›–è¾“å‡ºç›®å½•

```bash
python -m classifier.train output_dir=my_experiment
```

## ğŸ“– æ›´å¤šèµ„æº

- **è¯¦ç»†æ–‡æ¡£**: `classifier/README.md`
- **è®¾è®¡æ–‡æ¡£**: `CLASSIFIER_DESIGN.md`
- **ç¤ºä¾‹ä»£ç **: `classifier/example_usage.py`
- **é…ç½®æ–‡ä»¶**: `classifier/conf/`

## â“ å¸¸è§é—®é¢˜

### Q: CUDA out of memory

```bash
# å‡å° batch size
python -m classifier.train trainer.batch_size=16

# æˆ–ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
python -m classifier.train trainer.batch_size=8 trainer.gradient_accumulation_steps=4
```

### Q: æ•°æ®åŠ è½½å¤ªæ…¢

```bash
# å¢åŠ  workers
python -m classifier.train trainer.num_workers=8
```

### Q: æƒ³è¦ä½¿ç”¨è‡ªå·±çš„å›¾åƒ

```python
from classifier.data import ImageClassificationDataset
from classifier.data.convnext_processor import ConvNeXtProcessor

processor = ConvNeXtProcessor()
dataset = ImageClassificationDataset(
    image_paths=["path/to/img1.jpg", "path/to/img2.jpg", ...],
    labels=[0, 1, 2, ...],
    processor=processor,
)
```

## ğŸ‰ å¼€å§‹æ¢ç´¢

ç°åœ¨ä½ å·²ç»å‡†å¤‡å¥½äº†ï¼å¼€å§‹è®­ç»ƒä½ çš„æ¨¡å‹å§ï¼š

```bash
python -m classifier.train
```

ç¥è®­ç»ƒæ„‰å¿«ï¼ ğŸš€
